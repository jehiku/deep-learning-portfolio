{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d259748a-5993-488c-89f7-d932644a4f5f",
   "metadata": {},
   "source": [
    "# **Laboratory Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbcf782",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16364962-6454-4e2e-a4da-6279d37c821b",
   "metadata": {},
   "source": [
    "## **DS413 | Deep Learning**\n",
    "### **Understanding Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6268f6-1957-4cb7-8933-b7c1b75385e2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "In most neural networks, units in different layers are interconnected, with each connection having a weight that determines the influence of one unit on another. As data flows through these connections, the neural network progressively learns from the data, ultimately producing an output from the output layer. Artificial neural networks are trained using a dataset. To teach an ANN to recognize a cat, it is presented with thousands of different cat images. The network learns to identify cats by analyzing these images. Once trained, the ANN is tested by classifying new images and determining whether they are cat images or not. The output is compared to a human-provided label. If the ANN misclassifies an image, backpropagation is used to refine the network's weights based on the error rate. This process iterates until the ANN can accurately recognize cat images with minimal errors.\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb845a3-e4bc-4a49-8934-46b102dea1f6",
   "metadata": {},
   "source": [
    "<div style=\"width: 80%; margin: 0 auto;\">\n",
    "    <div style=\"border: 6px solid #4F6D38; padding: 15px; background-color: transparent; border-radius: 5px; text-align: left;\">\n",
    "       <a href=\"https://ibb.co/1YyCypBQ\"><img src=\"https://i.ibb.co/5XpQpzC4/image.png\" alt=\"image\" border=\"0\"></a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07a11c0a-c063-4a67-951a-d88eb54c0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23f85e89-bbd8-4a4d-a9e8-a346bf9f0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17490d3b-6a57-4db6-aad2-9cb5d7b5d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a94e579-1d91-4e7d-a5b8-818853f70c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 0.0, 1.0])\n",
    "\n",
    "W_hidden = np.array([[0.2, -0.3],\n",
    "                     [0.4,  0.1],\n",
    "                     [-0.5, 0.2]])\n",
    "\n",
    "b_hidden = np.array([-0.4, 0.2])\n",
    "\n",
    "W_out = np.array([-0.3, -0.2])\n",
    "\n",
    "b_out = 0.1\n",
    "\n",
    "y_true = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78027720-9950-4b1f-b7a6-4e8de57e3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass with ReLU\n",
    "\n",
    "Z_hidden = x.dot(W_hidden) + b_hidden  # weighted sums for hidden\n",
    "H = relu(Z_hidden)                      # ReLU activations for hidden\n",
    "Z_out = H.dot(W_out) + b_out            # output weighted sum\n",
    "y_hat = relu(Z_out)                     # ReLU output activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "909cc6fc-c7e0-4e02-9ed4-71f7b6fb92d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Error\n",
    "\n",
    "mse = 0.5 * (y_true - y_hat)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c09a814-7a84-4bf6-ae29-18cb5cfa766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_hidden = [-0.7  0.1]\n",
      "H (ReLU) = [0.  0.1]\n",
      "Z_out = 0.08\n",
      "y_hat (ReLU) = 0.08\n",
      "MSE = 0.4232\n"
     ]
    }
   ],
   "source": [
    "print(\"Z_hidden =\", Z_hidden)\n",
    "print(\"H (ReLU) =\", H)\n",
    "print(\"Z_out =\", Z_out)\n",
    "print(\"y_hat (ReLU) =\", y_hat)\n",
    "print(\"MSE =\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a62ef2-9209-44fb-a5e2-815694e0b307",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "In this activity, we walked through a single forward pass of a basic feedforward neural network. First, we calculated the linear combinations of the inputs with their corresponding weights and added the biases at the hidden layer. We then applied the ReLU activation function to introduce non-linearity. From there, the transformed values were used to compute the networkâ€™s final output. Finally, we evaluated the prediction by comparing it to the actual target using Mean Squared Error (MSE).\n",
    "\n",
    "This example clearly illustrates the sequence of operations within a neural network: data enters as inputs, is processed through weighted sums and biases, passes through an activation function, and produces an output that can be assessed through an error measure. It highlights how each component of the network plays a role in shaping predictions and in assessing how well the model performs.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
