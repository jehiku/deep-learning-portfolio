{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d259748a-5993-488c-89f7-d932644a4f5f",
   "metadata": {},
   "source": [
    "# **Laboratory Task 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601a1a6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16364962-6454-4e2e-a4da-6279d37c821b",
   "metadata": {},
   "source": [
    "## **DS413 | Deep Learning**\n",
    "### **Forward and Backward Propagation in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6268f6-1957-4cb7-8933-b7c1b75385e2",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify;\">\n",
    "This exercise introduced backpropagation as an extension of the feedforward neural network. After generating predictions through a forward pass with weighted sums and the ReLU activation function, the model’s performance was evaluated using Mean Squared Error (MSE). The error signal was then propagated backward to calculate gradients for both weights and biases. With these gradients, the parameters were refined through gradient descent using the set learning rate.\n",
    "    <br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb845a3-e4bc-4a49-8934-46b102dea1f6",
   "metadata": {},
   "source": [
    "<div style=\"width: 80%; margin: 0 auto;\">\n",
    "    <div style=\"border: 6px solid #4F6D38; padding: 15px; background-color: transparent; border-radius: 5px; text-align: left;\">\n",
    "    <h3><strong>Laboratory Task 1</strong></h3>\n",
    "    <p><strong>Instruction:</strong> From the example scenario above, create additional two inquiries for each type of data analytics.</p>\n",
    "\n",
    "x = np.array([1, 0, 1])\n",
    "\n",
    "y = np.array([1])\n",
    "\n",
    "**Use relu as the activation function.**\n",
    "\n",
    "Learning rate:\n",
    "\n",
    "lr = 0.001                                \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07a11c0a-c063-4a67-951a-d88eb54c0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01a97c84-9d7f-4349-9ccb-2253390de63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases (from Task 2)\n",
    "# Hidden layer weights (3 inputs -> 2 hidden neurons)\n",
    "\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],   # weights for input x1\n",
    "    [0.4,  0.1],   # weights for input x2\n",
    "    [-0.5, 0.2]    # weights for input x3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155817ba-67f9-477b-a655-d68ce8f35fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases (from Task 2)\n",
    "# Hidden layer weights (3 inputs -> 2 hidden neurons)\n",
    "\n",
    "W_hidden = np.array([\n",
    "    [0.2, -0.3],   # weights for input x1\n",
    "    [0.4,  0.1],   # weights for input x2\n",
    "    [-0.5, 0.2]    # weights for input x3\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e6869e5-fac5-4428-bebd-9f139f6f0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biases for hidden neurons\n",
    "b_hidden = np.array([-0.4, 0.2])\n",
    "\n",
    "# Output layer weights (2 hidden -> 1 output neuron)\n",
    "W_output = np.array([[-0.3], [-0.2]])\n",
    "\n",
    "# Bias for output neuron\n",
    "b_output = np.array([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b6f8f9-5677-46d3-a76e-d64f1d85f676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff8250-a89a-415b-a315-444e2ee6a8a0",
   "metadata": {},
   "source": [
    "---\n",
    "### **Feedforward:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0497ac19-5428-4d88-9e40-c858ae20f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "\n",
    "# Hidden layer computation\n",
    "Z_hidden = np.dot(x, W_hidden) + b_hidden   # weighted sum\n",
    "H = relu(Z_hidden)                         # apply ReLU\n",
    "\n",
    "# Output layer computation\n",
    "Z_output = np.dot(H, W_output) + b_output\n",
    "y_hat = relu(Z_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78f887-57d7-4eae-9db2-85f783bc707d",
   "metadata": {},
   "source": [
    "### **Feedforward Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97602e1d-2b3f-4747-b0a7-1f7f2b44ae18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "Z_hidden = [-0.7  0.1]\n",
      "H (hidden activations) = [0.  0.1]\n",
      "Z_output = [0.08]\n",
      "y_hat (prediction) = [0.08]\n"
     ]
    }
   ],
   "source": [
    "print(\"Forward Pass:\")\n",
    "print(\"Z_hidden =\", Z_hidden)\n",
    "print(\"H (hidden activations) =\", H)\n",
    "print(\"Z_output =\", Z_output)\n",
    "print(\"y_hat (prediction) =\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ccc0265-456a-4d8b-a397-97f084188a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.8464\n"
     ]
    }
   ],
   "source": [
    "# Compute loss (Mean Squared Error)\n",
    "\n",
    "loss = np.mean((y - y_hat) ** 2)\n",
    "print(\"Loss =\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4c3a4-876e-4381-9139-469d40f5f222",
   "metadata": {},
   "source": [
    "---\n",
    "### **Backward Propagation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c94bd2c8-2ef7-4943-8a61-a833f38ee638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "\n",
    "# Derivative of loss w.r.t y_hat (MSE derivative)\n",
    "dL_dyhat = 2 * (y_hat - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8065262e-38f2-4a0f-8990-8982041c98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative through ReLU at output\n",
    "dyhat_dZout = relu_derivative(Z_output)\n",
    "dL_dZout = dL_dyhat * dyhat_dZout\n",
    "\n",
    "# Gradients for output weights and bias\n",
    "dL_dWout = H.reshape(-1,1) @ dL_dZout.reshape(1,-1)   # outer product\n",
    "dL_dbout = dL_dZout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8de5b67e-47ed-49da-b538-f8d8e4c09dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for output weights and bias\n",
    "dL_dWout = H.reshape(-1,1) @ dL_dZout.reshape(1,-1)   # outer product\n",
    "dL_dbout = dL_dZout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16eb6631-e5bf-4453-9bd0-4004430f3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop to hidden layer\n",
    "dL_dH = dL_dZout @ W_output.T\n",
    "dH_dZhidden = relu_derivative(Z_hidden)\n",
    "dL_dZhidden = dL_dH * dH_dZhidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7b8db82-a247-4338-94a9-d134a0661dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients for hidden weights and biases\n",
    "dL_dWhidden = x.reshape(-1,1) @ dL_dZhidden.reshape(1,-1)\n",
    "dL_dbhidden = dL_dZhidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d739e54-8130-41f4-8de3-7f33d7e07eb1",
   "metadata": {},
   "source": [
    "### **Backward Propagation Results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79be7586-a2cd-49dc-966d-d339b6d88eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Pass:\n",
      "dL_dWout = \n",
      " [[ 0.   ]\n",
      " [-0.184]]\n",
      "dL_dbout = \n",
      " [-1.84]\n",
      "dL_dWhidden = \n",
      " [[0.    0.368]\n",
      " [0.    0.   ]\n",
      " [0.    0.368]]\n",
      "dL_dbhidden = \n",
      " [0.    0.368]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBackward Pass:\")\n",
    "print(\"dL_dWout =\", \"\\n\",dL_dWout)\n",
    "print(\"dL_dbout =\", \"\\n\", dL_dbout)\n",
    "print(\"dL_dWhidden =\",\"\\n\", dL_dWhidden)\n",
    "print(\"dL_dbhidden =\",\"\\n\", dL_dbhidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5c918ed-2304-4e13-a967-8a9a019b15d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights (Gradient Descent)\n",
    "\n",
    "W_output -= lr * dL_dWout\n",
    "b_output -= lr * dL_dbout\n",
    "W_hidden -= lr * dL_dWhidden\n",
    "b_hidden -= lr * dL_dbhidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e347b-b139-4745-ace3-75a2524f2865",
   "metadata": {},
   "source": [
    "---\n",
    "### **Updated Weights: Final Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9cac944-fcff-42ed-8601-c9cb7190d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Parameters:\n",
      "W_hidden = \n",
      " [[ 0.2      -0.300736]\n",
      " [ 0.4       0.1     ]\n",
      " [-0.5       0.199264]]\n",
      "b_hidden = \n",
      " [-0.4       0.199264]\n",
      "W_output = \n",
      " [[-0.3     ]\n",
      " [-0.199632]]\n",
      "b_output = \n",
      " [0.10368]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUpdated Parameters:\")\n",
    "print(\"W_hidden =\",\"\\n\", W_hidden)\n",
    "print(\"b_hidden =\",\"\\n\", b_hidden)\n",
    "print(\"W_output =\", \"\\n\",W_output)\n",
    "print(\"b_output =\",\"\\n\", b_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b429d51-08de-4857-ba4d-bcde81b07e70",
   "metadata": {},
   "source": [
    "---\n",
    "### **Conclusion**\n",
    "\n",
    "<div style=\"text-align: justify;\">\n",
    "The exercise focused on understanding how a feedforward neural network both processes information and learns from its mistakes. Predictions were first produced through a forward pass, where weighted inputs were combined, passed through a ReLU activation, and used to generate an output. The difference between this prediction and the target was then measured using Mean Squared Error (MSE). With that error as feedback, backpropagation was applied to compute gradients of the loss with respect to each weight and bias using the chain rule. Finally, these parameters were updated through gradient descent. Taken together, the steps revealed the full learning mechanism of a neural network—how it predicts, evaluates, and adapts to improve performance over time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37592ea-d2cb-4173-8a14-cccb90f5d757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
